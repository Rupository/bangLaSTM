{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45552866",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f351bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "# import whatever you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcd904",
   "metadata": {},
   "source": [
    "Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbbd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_embeddings = FastText.load(\"embeddings/cc.bn.300.model\")\n",
    "#chosen_embeddings = FastText.load(\"embeddings/ai4b_subset_sg.model\")\n",
    "#chosen_embeddings = FastText.load(\"embeddings/ai4b_subset_fair.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d7dae",
   "metadata": {},
   "source": [
    "Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1485027\n"
     ]
    }
   ],
   "source": [
    "def build_simple_embedding(gensim_model, keep_n = 150000):\n",
    "    wv = gensim_model.wv  \n",
    "    gensim_weights = torch.FloatTensor(wv.vectors[:keep_n])\n",
    "    # sorted, so keeping top 150000 works\n",
    "    \n",
    "    pad_weight = torch.zeros(1, wv.vector_size)                     # <PAD> gets zeros\n",
    "    special_weights = torch.randn(3, wv.vector_size) * 0.1          # <BOS>, <EOS>, <UNK> get random noise\n",
    "    # scale down (x0.1) to match sparseness of other token vecs\n",
    "\n",
    "    # combine <PAD>, <BOS>, <EOS> and <UNK> with other tokens\n",
    "    all_weights = torch.cat([pad_weight, special_weights, gensim_weights], dim=0)\n",
    "    \n",
    "    # make the full embedding\n",
    "    embedding_layer = nn.Embedding.from_pretrained(all_weights, freeze=False, padding_idx=0)\n",
    "    \n",
    "    # create mapping dictionary for token in new vocab, to index\n",
    "    word2idx = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    word2idx.update({word: idx + 4 for word, idx in wv.key_to_index.items()})\n",
    "    \n",
    "    return embedding_layer, word2idx\n",
    "\n",
    "embedding_layer, word_to_index = build_simple_embedding(chosen_embeddings)\n",
    "print('Vocabulary size:', len(word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af6370",
   "metadata": {},
   "source": [
    "(free up RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfb9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del chosen_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec663e",
   "metadata": {},
   "source": [
    "Encoder - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19e33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_size):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        \n",
    "        # AI agent - explain each of these, please!\n",
    "        self.embedding = embedding_layer # loads embedding made with gensim\n",
    "        self.hidden_size = hidden_size # neural net hidden size\n",
    "        embed_size = embedding_layer.embedding_dim # 300 for us\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, # 300\n",
    "            hidden_size=hidden_size, # suppose 256, for subsequent example\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # dry run: consider a batch (size 4) of question vectors, \n",
    "        # with the longest sequence's length as 8.\n",
    "            # [ 1, 45,  89,  12,  56,  90,  34,   2]\n",
    "            # [ 1, 19, 102,  77, 210,  14,   2,   0]\n",
    "            # [ 1, 65,  23,  11,   2,   0,   0,   0]\n",
    "            # [ 1, 99,  41,   2,   0,   0,   0,   0]\n",
    "        # this is loaded into the 2D tensor, x | shape: (4, 8)\n",
    "        # more generally, x | shape: (batch_size, longest_seq_len)\n",
    "\n",
    "        # length (1D) is the length of each sequence in x\n",
    "        # lengths = [8, 7, 5, 4] | shape: (4,)\n",
    "        # more generally, lengths | shape: (batch_size,)\n",
    "        \n",
    "\n",
    "        # convert each word index to its vector with the embedding. \n",
    "        # for our example, that's 4 sequences, with each 8 tokens each, and each\n",
    "        # token having a 'depth' of 300 (it's a vector now)\n",
    "\n",
    "        # this is a 3D tensor, embedded | shape: (4, 8, 300)\n",
    "        # more generally, embedded | shape: (batch_size, longest_seq_len, word_vec_embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # tells PyTorch to mathematically gloss over <PAD> tokens by ignoring them based on the \n",
    "        # values in the length vector (1D tensor)\n",
    "\n",
    "        # tells the neural net to fully ignore <PAD> tokens.\n",
    "        # even though they are zeroed out, the LSTM tries to do some math\n",
    "        # when encountering it using its 3 gates. this adds some redundancy \n",
    "        # and learning that it really doesn't need.\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, \n",
    "            lengths.cpu(), # [AI-agent: why CPU? # answer - test without CPU]\n",
    "            batch_first=True, # our formatting puts the batch_size first\n",
    "            enforce_sorted=False # sort the batch by sequence length (high to low)\n",
    "        )\n",
    "        \n",
    "        # run the nice embeddings through the BiLSTM\n",
    "        \n",
    "        # hidden --- final hidden state (short term memory) | shape: (2, 4, 256)\n",
    "        # 2 : forward + backward, 4 : sequences, 256 : hidden-size\n",
    "\n",
    "        # cell --- final cell state (long term memory) | shape: (2, 4, 256)\n",
    "        # (same logic)\n",
    "\n",
    "        _, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # hidden/cell tensors have shape (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # index 0 -> forward LSTM's final state; index 1 -> backward LSTM's final state\n",
    "\n",
    "        h_forward = hidden[0, :, :] # shape: (1, 4, 256) [take forward direction]\n",
    "        h_backward = hidden[1, :, :] # shape: (1, 4, 256) [take backward direction]\n",
    "        # recall, hidden | shape: (2, 4, 256)\n",
    "        \n",
    "        # same logic\n",
    "        c_forward = cell[0, :, :]\n",
    "        c_backward = cell[1, :, :]\n",
    "        \n",
    "        # concatenate along the hidden_size dimension (dim=1)\n",
    "        # h_context (c_context) | shape: (batch_size, hidden_size * 2) = (4, 256*2) = (4, 512)\n",
    "\n",
    "        h_context = torch.cat((h_forward, h_backward), dim=1)\n",
    "        c_context = torch.cat((c_forward, c_backward), dim=1)\n",
    "        \n",
    "        # compressing the context of each question (long term and short term)\n",
    "        # into two vectors of size 2*256 = 512, for every sentence in the batch\n",
    "        return h_context, c_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b835a8",
   "metadata": {},
   "source": [
    "Decoder - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a245068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_size, vocab_size):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        # same as before\n",
    "        self.embedding = embedding_layer\n",
    "        embed_size = embedding_layer.embedding_dim\n",
    "        \n",
    "        # double of BiLSTM hidden size\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # setup to hold hidden dim vectors streched out as probabilities\n",
    "        # over tokens in the vocab\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: input token for current step, shape: (batch_size) = (4) [suppose]\n",
    "        \n",
    "        # LSTM requires 3D input: (batch_size, sequence_length, embed_size).\n",
    "        # since we process exactly 1 token at a time (per batch), the sequence_length is always 1\n",
    "        # shape: (4, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # convert each token to its vector\n",
    "        # embedded | shape: (4, 1, 300)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # pass embedded word and BiLSTM question contexts (long term, short term) into LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output shape: (batch_size, 1, hidden_size) = (4, 1, 512)\n",
    "\n",
    "        # squeeze out the sequence length dimension because it is no longer needed\n",
    "        # shape: (4, 512)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        # push to linear layer to make prediction for current word\n",
    "        # shape: (batch_size, vocab_size) = (4, 1485027)\n",
    "        prediction = self.fc(output)\n",
    "        \n",
    "        # return guess and forward directional memory for next word\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2b5ea",
   "metadata": {},
   "source": [
    "Seq2Seq setup with Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        \n",
    "        # src: (batch_size, max_src_len) - padded bangla questions\n",
    "        # src_lengths: (batch_size) - true lengths of the questions\n",
    "        # trg: (batch_size, max_trg_len) - ground truth bangla answers\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        max_trg_len = trg.shape[1]\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        # empty tensor to hold word by word predictions\n",
    "        # outputs | shape: (batch_size, max_trg_len, vocab_size)\n",
    "        outputs = torch.zeros(batch_size, max_trg_len, vocab_size).to(self.device)\n",
    "        # shape: (4, 8, 1485027)\n",
    "        \n",
    "        # encode question\n",
    "        h_context, c_context = self.encoder(src, src_lengths)\n",
    "        \n",
    "        # format for decoder: (batch_size, 512) -> (1, batch_size, 512)\n",
    "        # LSTM class only accepts in this format\n",
    "        hidden = h_context.unsqueeze(0)\n",
    "        cell = c_context.unsqueeze(0)\n",
    "        \n",
    "        # first input to the decoder is ALWAYS the <BOS> token.\n",
    "        input_token = trg[:, 0]\n",
    "        # this is a column vector of <BOS> tokens, [<BOS>]\n",
    "        \n",
    "        # from first word onwards... \n",
    "        for t in range(1, max_trg_len):\n",
    "            \n",
    "            # pass the current word and the memory states into the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            # shape: (4, 1485027)\n",
    "            \n",
    "            # store the prediction in our outputs tensor\n",
    "            outputs[:, t, :] = output\n",
    "            # t'th word across all batches and full vocabulary has been saved as output\n",
    "            \n",
    "            # TEACHER FORCING: \n",
    "            # ignore whatever the model output.\n",
    "            # force the next input to be the TRUE target token from the dataset.\n",
    "            input_token = trg[:, t] \n",
    "            # take next column of true answer words as input\n",
    "            \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
