{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9LzHe-w_if5u",
   "metadata": {
    "id": "9LzHe-w_if5u"
   },
   "source": [
    "#### Connect Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZcwLXa5imPk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 5527,
     "status": "ok",
     "timestamp": 1771931433520,
     "user": {
      "displayName": "Ritwik Ray",
      "userId": "11041256575692089057"
     },
     "user_tz": -330
    },
    "id": "NZcwLXa5imPk",
    "outputId": "93c9856d-cb33-4a62-9c36-d58750f4d151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/1PnMuZB2WBL9gApz52tmg-tlFJkJ8O4vn/bangLaSTM'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "os.chdir('/content/drive/MyDrive/Colab/bangLaSTM')\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45552866",
   "metadata": {
    "id": "45552866"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f351bf0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7f351bf0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcd904",
   "metadata": {
    "id": "02dcd904"
   },
   "source": [
    "Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbbd531",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2fbbd531"
   },
   "outputs": [],
   "source": [
    "#chosen_embeddings = FastText.load(\"embeddings/cc.bn.300.model\")\n",
    "#chosen_embeddings = FastText.load(\"embeddings/ai4b_subset_sg.model\")\n",
    "chosen_embeddings = FastText.load(\"embeddings/ai4b_subset_fair.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d7dae",
   "metadata": {
    "id": "fa7d7dae"
   },
   "source": [
    "Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3d5dfd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3b3d5dfd"
   },
   "outputs": [],
   "source": [
    "def build_simple_embedding(gensim_model:FastText, keep_n = 150000):\n",
    "    wv = gensim_model.wv\n",
    "    gensim_weights = torch.FloatTensor(wv.vectors[:keep_n])\n",
    "    # sorted, so keeping top 150000 works\n",
    "\n",
    "    pad_weight = torch.zeros(1, wv.vector_size)                     # <PAD> gets zeros\n",
    "    special_weights = torch.randn(3, wv.vector_size) * 0.1          # <BOS>, <EOS>, <UNK> get random noise\n",
    "    # scale down (x0.1) to match sparseness of other token vecs\n",
    "\n",
    "    # combine <PAD>, <BOS>, <EOS> and <UNK> with other tokens\n",
    "    all_weights = torch.cat([pad_weight, special_weights, gensim_weights], dim=0)\n",
    "\n",
    "    # make the full embedding\n",
    "    embedding_layer = nn.Embedding.from_pretrained(all_weights, freeze=False, padding_idx=0)\n",
    "\n",
    "    # create mapping dictionary for token in new vocab, to index\n",
    "    word2idx = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    for idx, word in enumerate(wv.index_to_key[:keep_n]):\n",
    "        word2idx[word] = idx + 4\n",
    "\n",
    "    return embedding_layer, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48eb0788",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1771931489017,
     "user": {
      "displayName": "Ritwik Ray",
      "userId": "11041256575692089057"
     },
     "user_tz": -330
    },
    "id": "48eb0788",
    "outputId": "8f0291cd-7439-4f40-96e7-d6005d6c5060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 150004\n"
     ]
    }
   ],
   "source": [
    "embedding_layer, word_to_index = build_simple_embedding(chosen_embeddings)\n",
    "print('Vocabulary size:', len(word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af6370",
   "metadata": {
    "id": "60af6370"
   },
   "source": [
    "(free up RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfb9af0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3dfb9af0"
   },
   "outputs": [],
   "source": [
    "del chosen_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVfxs5Yw6-VA",
   "metadata": {
    "id": "KVfxs5Yw6-VA"
   },
   "source": [
    "Setup tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gzltRhhn7AnK",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gzltRhhn7AnK"
   },
   "outputs": [],
   "source": [
    "def tokenize_bangla(text):\n",
    "    text_spaced = re.sub(r'([^\\u0980-\\u09FF\\u200C\\u200D0-9\\s])', r' \\1 ', str(text))\n",
    "    return text_spaced.split()\n",
    "\n",
    "def detokenize_bangla(word_list):\n",
    "    return \" \".join(word_list) # can't and won't handle punctuation\n",
    "    # because on reconstruction we have no way to reconcile stuff like close) and open( parenthesese\n",
    "    # or symmetric 'quote' vs. apostroph'e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec663e",
   "metadata": {
    "id": "d2ec663e"
   },
   "source": [
    "### Encoder - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d19e33b9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d19e33b9"
   },
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_size):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = embedding_layer # loads embedding made with gensim\n",
    "        self.hidden_size = hidden_size # neural net hidden size\n",
    "        embed_size = embedding_layer.embedding_dim # 300 for us\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size, # 300\n",
    "            hidden_size=hidden_size, # suppose 256, for subsequent example\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # dry run: consider a batch (size 4) of question vectors,\n",
    "        # with the longest sequence's length as 8.\n",
    "            # [ 1, 45,  89,  12,  56,  90,  34,   2]\n",
    "            # [ 1, 19, 102,  77, 210,  14,   2,   0]\n",
    "            # [ 1, 65,  23,  11,   2,   0,   0,   0]\n",
    "            # [ 1, 99,  41,   2,   0,   0,   0,   0]\n",
    "        # this is loaded into the 2D tensor, x | shape: (4, 8)\n",
    "        # more generally, x | shape: (batch_size, longest_seq_len)\n",
    "\n",
    "        # length (1D) is the length of each sequence in x\n",
    "        # lengths = [8, 7, 5, 4] | shape: (4,)\n",
    "        # more generally, lengths | shape: (batch_size,)\n",
    "\n",
    "\n",
    "        # convert each word index to its vector with the embedding.\n",
    "        # for our example, that's 4 sequences, with each 8 tokens each, and each\n",
    "        # token having a 'depth' of 300 (it's a vector now)\n",
    "\n",
    "        # this is a 3D tensor, embedded | shape: (4, 8, 300)\n",
    "        # more generally, embedded | shape: (batch_size, longest_seq_len, word_vec_embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # tells PyTorch to mathematically gloss over <PAD> tokens by ignoring them based on the\n",
    "        # values in the length vector (1D tensor)\n",
    "\n",
    "        # tells the neural net to fully ignore <PAD> tokens.\n",
    "        # even though they are zeroed out, the LSTM tries to do some math\n",
    "        # when encountering it using its 3 gates. this adds some redundancy\n",
    "        # and learning that it really doesn't need.\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True, # our formatting puts the batch_size first\n",
    "            enforce_sorted=False # sort the batch by sequence length (high to low)\n",
    "        )\n",
    "\n",
    "        # run the nice embeddings through the BiLSTM\n",
    "\n",
    "        # hidden --- final hidden state (short term memory) | shape: (2, 4, 256)\n",
    "        # 2 : forward + backward, 4 : sequences, 256 : hidden-size\n",
    "\n",
    "        # cell --- final cell state (long term memory) | shape: (2, 4, 256)\n",
    "        # (same logic)\n",
    "\n",
    "        _, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # hidden/cell tensors have shape (num_layers * num_directions, batch_size, hidden_size)\n",
    "        # index 0 -> forward LSTM's final state; index 1 -> backward LSTM's final state\n",
    "\n",
    "        h_forward = hidden[0, :, :] # shape: (1, 4, 256) [take forward direction]\n",
    "        h_backward = hidden[1, :, :] # shape: (1, 4, 256) [take backward direction]\n",
    "        # recall, hidden | shape: (2, 4, 256)\n",
    "\n",
    "        # same logic\n",
    "        c_forward = cell[0, :, :]\n",
    "        c_backward = cell[1, :, :]\n",
    "\n",
    "        # concatenate along the hidden_size dimension (dim=1)\n",
    "        # h_context (c_context) | shape: (batch_size, hidden_size * 2) = (4, 256*2) = (4, 512)\n",
    "\n",
    "        h_context = torch.cat((h_forward, h_backward), dim=1)\n",
    "        c_context = torch.cat((c_forward, c_backward), dim=1)\n",
    "\n",
    "        # compressing the context of each question (long term and short term)\n",
    "        # into two vectors of size 2*256 = 512, for every sentence in the batch\n",
    "        return h_context, c_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b835a8",
   "metadata": {
    "id": "69b835a8"
   },
   "source": [
    "### Decoder - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a245068",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1a245068"
   },
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_size, vocab_size):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        # same as before\n",
    "        self.embedding = embedding_layer\n",
    "        embed_size = embedding_layer.embedding_dim\n",
    "\n",
    "        # double of BiLSTM hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # setup to hold hidden dim vectors streched out as probabilities\n",
    "        # over tokens in the vocab\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: input token for current step, shape: (batch_size) = (4) [suppose]\n",
    "\n",
    "        # LSTM requires 3D input: (batch_size, sequence_length, embed_size).\n",
    "        # since we process exactly 1 token at a time (per batch), the sequence_length is always 1\n",
    "        # shape: (4, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # convert each token to its vector\n",
    "        # embedded | shape: (4, 1, 300)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # pass embedded word and BiLSTM question contexts (long term, short term) into LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output shape: (batch_size, 1, hidden_size) = (4, 1, 512)\n",
    "\n",
    "        # squeeze out the sequence length dimension because it is no longer needed\n",
    "        # shape: (4, 512)\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        # push to linear layer to make prediction for current word\n",
    "        # shape: (batch_size, vocab_size) = (4, 1485027)\n",
    "        prediction = self.fc(output)\n",
    "\n",
    "        # return guess and forward directional memory for next word\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2b5ea",
   "metadata": {
    "id": "99d2b5ea"
   },
   "source": [
    "### Seq2Seq setup with Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13f588f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f13f588f"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, src_lengths, trg, teach_prob=0.5):\n",
    "\n",
    "        # src: (batch_size, max_src_len) - padded bangla questions\n",
    "        # src_lengths: (batch_size) - true lengths of the questions\n",
    "        # trg: (batch_size, max_trg_len) - ground truth bangla answers\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        max_trg_len = trg.shape[1]\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # empty tensor to hold word by word predictions\n",
    "        # outputs | shape: (batch_size, max_trg_len, vocab_size)\n",
    "        outputs = torch.zeros(batch_size, max_trg_len, vocab_size, device=self.device)\n",
    "        # shape: (4, 8, 1485027)\n",
    "\n",
    "        # encode question\n",
    "        h_context, c_context = self.encoder(src, src_lengths)\n",
    "\n",
    "        # format for decoder: (batch_size, 512) -> (1, batch_size, 512)\n",
    "        # LSTM class only accepts in this format\n",
    "        hidden = h_context.unsqueeze(0)\n",
    "        cell = c_context.unsqueeze(0)\n",
    "\n",
    "        # first input to the decoder is ALWAYS the <BOS> token.\n",
    "        input_token = trg[:, 0]\n",
    "        # this is a column vector of <BOS> tokens, [<BOS>]\n",
    "\n",
    "        # from first word onwards...\n",
    "        for t in range(1, max_trg_len):\n",
    "\n",
    "            # pass the current word and the memory states into the decoder\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            # shape: (4, 1485027)\n",
    "\n",
    "            # store the prediction in our outputs tensor\n",
    "            outputs[:, t, :] = output\n",
    "            # t'th word across all batches and full vocabulary has been saved as output\n",
    "\n",
    "            should_teach = random.random() < teach_prob\n",
    "            # TEACHER FORCING: - 50% of the time\n",
    "            # ignore whatever the model outputs\n",
    "            # force the next input to be the TRUE target token from the dataset.\n",
    "            input_token = trg[:, t] if should_teach else output.argmax(1)\n",
    "            # take next column of true answer words as input\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b09ad",
   "metadata": {
    "id": "324b09ad"
   },
   "source": [
    "Data wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bb9dc9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c7bb9dc9"
   },
   "outputs": [],
   "source": [
    "class BanglaQADataset(Dataset):\n",
    "    def __init__(self, dataframe, word2idx):\n",
    "\n",
    "        self.questions = dataframe['question'].tolist()\n",
    "        self.answers = dataframe['answer'].tolist()\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "        self.unk_idx = word2idx['<UNK>']\n",
    "        self.bos_idx = word2idx['<BOS>']\n",
    "        self.eos_idx = word2idx['<EOS>']\n",
    "\n",
    "    def tokenize_and_map(self, sentence):\n",
    "        # tokenise the same way as word vecs\n",
    "        tokens = tokenize_bangla(sentence)\n",
    "\n",
    "        # map words to integers, use <UNK> (index 3) if not found\n",
    "        indices = [self.word2idx.get(token, self.unk_idx) for token in tokens]\n",
    "        # wrap with <BOS> and <EOS>\n",
    "        sequence = [self.bos_idx] + indices + [self.eos_idx]\n",
    "\n",
    "        return torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_tensor = self.tokenize_and_map(self.questions[idx])\n",
    "        a_tensor = self.tokenize_and_map(self.answers[idx])\n",
    "        return q_tensor, a_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "    lengths = []\n",
    "\n",
    "    for q, a in batch:\n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "        lengths.append(len(q))\n",
    "\n",
    "    # pad the sequences with 0 (<PAD>)\n",
    "    # batch_first=True makes the output shape (batch_size, max_seq_length)\n",
    "    padded_questions = pad_sequence(questions, batch_first=True, padding_value=0)\n",
    "    padded_answers = pad_sequence(answers, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Convert lengths to a tensor\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return padded_questions, lengths_tensor, padded_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe154a",
   "metadata": {
    "id": "0cfe154a"
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae10aa",
   "metadata": {
    "id": "e6ae10aa"
   },
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bf8908",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1771931489411,
     "user": {
      "displayName": "Ritwik Ray",
      "userId": "11041256575692089057"
     },
     "user_tz": -330
    },
    "id": "76bf8908",
    "outputId": "c3c22f46-aa40-486d-a433-37b4963d296e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "seed_generator = torch.Generator()\n",
    "seed_generator.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ENC_HIDDEN_DIM = 256\n",
    "DEC_HIDDEN_DIM = 512\n",
    "VOCAB_SIZE = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GbQTSpow1mco",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GbQTSpow1mco"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('data/question_answer/subsets/bn_train.parquet')\n",
    "train_dataset = BanglaQADataset(df_train, word_to_index)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    generator=seed_generator,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "df_val = pd.read_parquet('data/question_answer/subsets/bn_val.parquet')\n",
    "val_dataset = BanglaQADataset(df_val, word_to_index)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lIndxLEevgSN",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lIndxLEevgSN"
   },
   "outputs": [],
   "source": [
    "enc = BiLSTMEncoder(embedding_layer, ENC_HIDDEN_DIM)\n",
    "dec = LSTMDecoder(embedding_layer, DEC_HIDDEN_DIM, VOCAB_SIZE)\n",
    "seq2seq_model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001, weight_decay=5e-5)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "pad_idx = word_to_index['<PAD>']\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7798bf",
   "metadata": {
    "id": "1f7798bf"
   },
   "source": [
    "Train and Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50858ef",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f50858ef"
   },
   "outputs": [],
   "source": [
    "def train_epoch(seq2seq_model, iterator, optimizer, loss_func, scaler, clip=1.0):\n",
    "    seq2seq_model.train() # train time\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, src_lengths, trg in tqdm(iterator, desc=\"Training\"):\n",
    "        src, src_lengths, trg = src.to(device), src_lengths.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = seq2seq_model(src, src_lengths, trg, teach_prob = 0.5) # partial teacher forcing enabled\n",
    "\n",
    "            # torch's CE loss accepts things only in a certain format. the\n",
    "            # pre loss calculation block is formatting it\n",
    "\n",
    "            # output | shape: (batch_size, seq_len, vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            # get vocab size, 150004\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            # [:, 1:, :] -- take everything except [<BOS>] column\n",
    "            # reshape -- convert into a (batch_size * seq_len) list of predictions (150004 options)\n",
    "            # **technically (seq_len - 1) because we ignore [<BOS>]\n",
    "            # same logic with ground truth, except no predictions\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # calculate loss and backpropagate\n",
    "            loss = loss_func(output, trg)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq_model.parameters(), clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_epoch(seq2seq_model, iterator, loss_func):\n",
    "    seq2seq_model.eval() # eval mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad(): # disable gradient tracking to save RAM\n",
    "        for src, src_lengths, trg in tqdm(iterator, desc='Validating'):\n",
    "            src, src_lengths, trg = src.to(device), src_lengths.to(device), trg.to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "              output = seq2seq_model(src, src_lengths, trg, teach_prob = 0) # teacher forcing disabled\n",
    "\n",
    "              output_dim = output.shape[-1]\n",
    "              output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "              trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "              loss = loss_func(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VEgBfmILnKP-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203,
     "referenced_widgets": [
      "ee11a77083904ccf86dc9c7e70940748",
      "91aa3cc8fd824d7c9d67fc47a45396cc",
      "73d13d4758b840c68386be90a4158791",
      "3db2136a1eb3420ea4d0f5593b504400",
      "3a9499c0ba764c5d9db6a94791473034",
      "a5eb7bbfe7ae477ca74aeebc69c1507f",
      "b8dbace98bc847078f0a691807597bde",
      "dc84f4d992304147b177f9742c5cfe1c",
      "4841ffc5070a4c39b26ea485ab446eca",
      "230d0ee2228948bbb41555211053880e",
      "5a28879cebae4b7db12b4e346564fb3a",
      "1b954ba04fa64684a890f2e46d3bac9c",
      "2a3c71364b4e4bbe97e67aed7268565a",
      "11650ca1cd234a978a0f176731c7eb52",
      "fa8a07dd25d14eff987c7dc2338ccaff",
      "be7b994f49904eb48f82b8a02976b68d",
      "2422aceaa85841ab92ef74c65f360f08",
      "0ca51d1b8e5442a5a27fe7c96ff41e41",
      "011eecd16ad4459dbf7319d36731b959",
      "761f7248188c421a80d3df6708bff257",
      "3ff600a5399343419cc2af42b2487aa9",
      "aa771bcc5af54344b2aa05113bec9d34",
      "7f6be6bcf1b44b30901fcbc54811ef37"
     ]
    },
    "id": "VEgBfmILnKP-",
    "outputId": "de16b881-bdb0-4698-ab12-0d106ff109e1"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n--- Epoch {epoch+1} ---')\n",
    "\n",
    "    train_loss = train_epoch(seq2seq_model, train_loader, optimizer, loss_func, scaler)\n",
    "    valid_loss = evaluate_epoch(seq2seq_model, val_loader, loss_func)\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Loss: {valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6Pf80qoTVbI",
   "metadata": {
    "id": "E6Pf80qoTVbI"
   },
   "source": [
    "Save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W1kcCtOuIS4X",
   "metadata": {
    "id": "W1kcCtOuIS4X"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': seq2seq_model.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'models/ai4b_qna_sg_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usUSH8qBUR3u",
   "metadata": {
    "id": "usUSH8qBUR3u"
   },
   "source": [
    "### Metrics & Inference\n",
    "\n",
    "Rerun any necessary cells from previous setup if running this for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "xpaZAaAewNxd",
   "metadata": {
    "id": "xpaZAaAewNxd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "g5G9WcY5uVGy",
   "metadata": {
    "id": "g5G9WcY5uVGy"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('data/question_answer/subsets/bn_test.parquet')\n",
    "\n",
    "test_dataset = BanglaQADataset(df_test, word_to_index)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "g_5cPhvPUWOM",
   "metadata": {
    "id": "g_5cPhvPUWOM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): BiLSTMEncoder(\n",
       "    (embedding): Embedding(150004, 300, padding_idx=0)\n",
       "    (lstm): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (embedding): Embedding(150004, 300, padding_idx=0)\n",
       "    (lstm): LSTM(300, 512, batch_first=True)\n",
       "    (fc): Linear(in_features=512, out_features=150004, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('models/ai4b_qna_sg_model.pt', map_location=device, weights_only=False)\n",
    "\n",
    "word_to_index = checkpoint['word_to_index']\n",
    "embedding_layer = checkpoint['embedding_layer']\n",
    "vocab_size = len(word_to_index)\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "model = Seq2Seq(\n",
    "    BiLSTMEncoder(embedding_layer, ENC_HIDDEN_DIM),\n",
    "    LSTMDecoder(embedding_layer, DEC_HIDDEN_DIM, vocab_size),\n",
    "    device\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ofuz4_dKxi9P",
   "metadata": {
    "id": "Ofuz4_dKxi9P"
   },
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Wos4sXoPVkDD",
   "metadata": {
    "id": "Wos4sXoPVkDD"
   },
   "outputs": [],
   "source": [
    "def generate_answer(model, question, word_to_index, index_to_word, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenize_bangla(question)\n",
    "    indices = [word_to_index['<BOS>']] + \\\n",
    "              [word_to_index.get(word, word_to_index['<UNK>']) for word in tokens] + \\\n",
    "              [word_to_index['<EOS>']]\n",
    "\n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "    src_len = torch.LongTensor([len(indices)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h, c = model.encoder(src_tensor, src_len)\n",
    "        hidden = h.unsqueeze(0)\n",
    "        cell = c.unsqueeze(0)\n",
    "\n",
    "        input_token = torch.LongTensor([word_to_index['<BOS>']]).to(device)\n",
    "        result = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "            top_token = output.argmax(1)\n",
    "\n",
    "            if top_token.item() == word_to_index['<EOS>']:\n",
    "                break\n",
    "\n",
    "            result.append(index_to_word.get(top_token.item(), '<UNK>'))\n",
    "            input_token = top_token\n",
    "\n",
    "    return detokenize_bangla(result)\n",
    "\n",
    "\n",
    "def generate_answer_batched(model, src_tensor, src_len, word_to_index, index_to_word, device, max_len=50):\n",
    "    model.eval()\n",
    "    batch_size = src_tensor.shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h, c = model.encoder(src_tensor, src_len)\n",
    "        hidden = h.unsqueeze(0)\n",
    "        cell = c.unsqueeze(0)\n",
    "\n",
    "        input_token = torch.full((batch_size,), word_to_index['<BOS>'], dtype=torch.long, device=device)\n",
    "        batch_results = [[] for _ in range(batch_size)]\n",
    "\n",
    "        unfinished = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "\n",
    "            top_tokens = output.argmax(1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if unfinished[i]:\n",
    "                    token_id = top_tokens[i].item()\n",
    "                    if token_id == word_to_index['<EOS>']:\n",
    "                        unfinished[i] = False\n",
    "                    else:\n",
    "                        batch_results[i].append(index_to_word.get(token_id, '<UNK>'))\n",
    "\n",
    "            if not unfinished.any():\n",
    "                break\n",
    "\n",
    "            input_token = top_tokens\n",
    "\n",
    "    return [detokenize_bangla(res) for res in batch_results]\n",
    "\n",
    "def generate_stepwise(model, question, word_to_index, index_to_word, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenize_bangla(question)\n",
    "    indices = [word_to_index['<BOS>']] + \\\n",
    "              [word_to_index.get(word, word_to_index['<UNK>']) for word in tokens] + \\\n",
    "              [word_to_index['<EOS>']]\n",
    "\n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "    src_len = torch.LongTensor([len(indices)])\n",
    "\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h, c = model.encoder(src_tensor, src_len)\n",
    "        hidden = h.unsqueeze(0)\n",
    "        cell = c.unsqueeze(0)\n",
    "\n",
    "        input_token = torch.LongTensor([word_to_index['<BOS>']]).to(device)\n",
    "        result = []\n",
    "\n",
    "        for step in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            top_probs, top_indices = torch.topk(probs, 5, dim=1)\n",
    "\n",
    "            print(f\"--- Token {step + 1} ---\")\n",
    "            for i in range(5):\n",
    "                word = index_to_word.get(top_indices[0][i].item(), '<UNK>')\n",
    "                prob = top_probs[0][i].item() * 100\n",
    "                print(f\"  {i+1}. {word} ({prob:.2f}%)\")\n",
    "\n",
    "            top_token = output.argmax(1)\n",
    "            chosen_word = index_to_word.get(top_token.item(), '<UNK>')\n",
    "            print(f\">> Model chose: '{chosen_word}'\\n\")\n",
    "\n",
    "            if top_token.item() == word_to_index['<EOS>']:\n",
    "                print(\">> Reached <EOS>.\")\n",
    "                break\n",
    "\n",
    "            result.append(chosen_word)\n",
    "            input_token = top_token\n",
    "\n",
    "            user_input = input(\"Proceed? [ENTER/q]\\n\")\n",
    "            if user_input.lower() == 'q':\n",
    "                print(\"\\n>> Stopped.\")\n",
    "                break\n",
    "            print()\n",
    "\n",
    "    return detokenize_bangla(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NaB7Ziqtxxh-",
   "metadata": {
    "id": "NaB7Ziqtxxh-"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "r8jCFVjKxwVe",
   "metadata": {
    "id": "r8jCFVjKxwVe"
   },
   "outputs": [],
   "source": [
    "def get_metrics(predicted_sentence, truth_sentence):\n",
    "  pred_tokens = tokenize_bangla(predicted_sentence)\n",
    "  truth_tokens = tokenize_bangla(truth_sentence)\n",
    "\n",
    "  exact_match = pred_tokens == truth_tokens\n",
    "  # (almost exact match. we can't handle assymmetric vs symmetric punctuation\n",
    "  # with our tokenization method. think of an apostroph'e and a 'quote', or a opening parentheses vs a closing)\n",
    "\n",
    "\n",
    "  if not pred_tokens or not truth_tokens:\n",
    "      return (0.0, 0.0, 0.0, exact_match)\n",
    "\n",
    "  common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "  num_same = sum(common.values())\n",
    "\n",
    "  precision = num_same / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "  recall = num_same / len(truth_tokens) if len(truth_tokens) > 0 else 0\n",
    "  f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "  return precision, recall, f1, exact_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-zRwImR8CNMH",
   "metadata": {
    "id": "-zRwImR8CNMH"
   },
   "source": [
    "GPU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4O3-g5qDCMBw",
   "metadata": {
    "id": "4O3-g5qDCMBw"
   },
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "for src_tensor, src_len, _ in tqdm(test_loader, desc=\"Inference\"):\n",
    "    src_tensor = src_tensor.to(device)\n",
    "    src_len = src_len.to(device)\n",
    "\n",
    "    batch_preds = generate_answer_batched(model, src_tensor, src_len, word_to_index, index_to_word, device)\n",
    "    all_predictions.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xQQs0c9kCRGY",
   "metadata": {
    "id": "xQQs0c9kCRGY"
   },
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TU5JU92iVmV7",
   "metadata": {
    "id": "TU5JU92iVmV7"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for pred, truth in zip(all_predictions, df_test['answer']):\n",
    "    metrics = get_metrics(pred, truth)\n",
    "    results.append(metrics)\n",
    "\n",
    "p, r, f1, em = zip(*results)\n",
    "print(f\"\\n--- Final Performance ---\")\n",
    "print(f\"Precision:   {sum(p)/len(p):.4f}\")\n",
    "print(f\"Recall:      {sum(r)/len(r):.4f}\")\n",
    "print(f\"F1-Score:    {sum(f1)/len(f1):.4f}\")\n",
    "print(f\"Exact Match: {sum(em)/len(em)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "t09sLtxlyD4D",
   "metadata": {
    "id": "t09sLtxlyD4D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: দিল্লি কোন দেশের রাজধানী?\n",
      "\n",
      "--- Token 1 ---\n",
      "  1. পাকিস্তান (7.59%)\n",
      "  2. দক্ষিণ (7.27%)\n",
      "  3. বাংলাদেশ (5.21%)\n",
      "  4. আফগানিস্তান (4.92%)\n",
      "  5. <UNK> (3.12%)\n",
      ">> Model chose: 'পাকিস্তান'\n",
      "\n",
      "\n",
      "--- Token 2 ---\n",
      "  1. । (98.25%)\n",
      "  2. আরব (0.25%)\n",
      "  3. যুক্তরাষ্ট্র (0.20%)\n",
      "  4. ও (0.18%)\n",
      "  5. - (0.14%)\n",
      ">> Model chose: '।'\n",
      "\n",
      "\n",
      "--- Token 3 ---\n",
      "  1. <EOS> (97.57%)\n",
      "  2. । (1.90%)\n",
      "  3. আমিরাত (0.11%)\n",
      "  4. টোবাগো (0.06%)\n",
      "  5. <UNK> (0.03%)\n",
      ">> Model chose: '<EOS>'\n",
      "\n",
      ">> Reached <EOS>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'পাকিস্তান ।'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question = 'দিল্লি কোন দেশের রাজধানী?'\n",
    "generate_stepwise(model, sample_question, word_to_index, index_to_word, device)\n",
    "#generate_answer(model, sample_question, word_to_index, index_to_word, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011eecd16ad4459dbf7319d36731b959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ca51d1b8e5442a5a27fe7c96ff41e41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11650ca1cd234a978a0f176731c7eb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ca51d1b8e5442a5a27fe7c96ff41e41",
      "placeholder": "​",
      "style": "IPY_MODEL_011eecd16ad4459dbf7319d36731b959",
      "value": "Training:  30%"
     }
    },
    "1b954ba04fa64684a890f2e46d3bac9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "230d0ee2228948bbb41555211053880e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2422aceaa85841ab92ef74c65f360f08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a3c71364b4e4bbe97e67aed7268565a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11650ca1cd234a978a0f176731c7eb52",
       "IPY_MODEL_fa8a07dd25d14eff987c7dc2338ccaff",
       "IPY_MODEL_be7b994f49904eb48f82b8a02976b68d"
      ],
      "layout": "IPY_MODEL_2422aceaa85841ab92ef74c65f360f08"
     }
    },
    "3a9499c0ba764c5d9db6a94791473034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a28879cebae4b7db12b4e346564fb3a",
      "placeholder": "​",
      "style": "IPY_MODEL_1b954ba04fa64684a890f2e46d3bac9c",
      "value": " 239/239 [00:13&lt;00:00, 16.85it/s]"
     }
    },
    "3db2136a1eb3420ea4d0f5593b504400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4841ffc5070a4c39b26ea485ab446eca",
      "max": 239,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_230d0ee2228948bbb41555211053880e",
      "value": 239
     }
    },
    "3ff600a5399343419cc2af42b2487aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4841ffc5070a4c39b26ea485ab446eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a28879cebae4b7db12b4e346564fb3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d13d4758b840c68386be90a4158791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8dbace98bc847078f0a691807597bde",
      "placeholder": "​",
      "style": "IPY_MODEL_dc84f4d992304147b177f9742c5cfe1c",
      "value": "Validating: 100%"
     }
    },
    "761f7248188c421a80d3df6708bff257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f6be6bcf1b44b30901fcbc54811ef37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91aa3cc8fd824d7c9d67fc47a45396cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73d13d4758b840c68386be90a4158791",
       "IPY_MODEL_3db2136a1eb3420ea4d0f5593b504400",
       "IPY_MODEL_3a9499c0ba764c5d9db6a94791473034"
      ],
      "layout": "IPY_MODEL_a5eb7bbfe7ae477ca74aeebc69c1507f"
     }
    },
    "a5eb7bbfe7ae477ca74aeebc69c1507f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa771bcc5af54344b2aa05113bec9d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8dbace98bc847078f0a691807597bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be7b994f49904eb48f82b8a02976b68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa771bcc5af54344b2aa05113bec9d34",
      "placeholder": "​",
      "style": "IPY_MODEL_7f6be6bcf1b44b30901fcbc54811ef37",
      "value": " 214/715 [01:10&lt;02:43,  3.07it/s]"
     }
    },
    "dc84f4d992304147b177f9742c5cfe1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa8a07dd25d14eff987c7dc2338ccaff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_761f7248188c421a80d3df6708bff257",
      "max": 715,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ff600a5399343419cc2af42b2487aa9",
      "value": 214
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
